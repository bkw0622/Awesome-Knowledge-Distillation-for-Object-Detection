# Knowledge-Distillation-for-Object-Detection

### [(CVPR2017) Mimicking very efficient network for object detection](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8100259)

<img src="https://user-images.githubusercontent.com/66883050/216758223-a1767b16-e3b4-42ab-ae23-d5ed9800dd96.png"  width="50%" height="50%"/>

### [(CVPR2019) Distilling object detectors with fine-grained feature imitation](https://arxiv.org/abs/1906.03609)

<img src="https://user-images.githubusercontent.com/66883050/216758237-e6a27300-73d0-43dc-8f36-c9a24ad84f92.png"  width="50%" height="50%"/>

### [(CVPR2021) General instance distillation for object detection](https://arxiv.org/abs/2103.02340)

<img src="https://user-images.githubusercontent.com/66883050/216758253-63da5c2d-659f-4072-af41-3fcbe0970c7f.png"  width="50%" height="50%"/>

### [(CVPR2021) Distilling object detectors via decoupled features](https://arxiv.org/abs/2103.14475)

<img src="https://user-images.githubusercontent.com/66883050/216758264-d4aa0073-4e45-4094-adac-870e889b6261.png"  width="50%" height="50%"/>

### [(NeurIPS2021) Distilling object detectors with feature richness](https://arxiv.org/abs/2111.00674)

<img src="https://user-images.githubusercontent.com/66883050/216758273-be09d30b-6184-4ef5-a591-e3d70ea5ea20.png"  width="50%" height="50%"/>

### [(CVPR2022) Focal and global knowledge distillation for detectors](https://arxiv.org/abs/2111.11837)

<img src="https://user-images.githubusercontent.com/66883050/216758284-215859ae-68e9-4f0d-9764-09b0c22484ca.png"  width="50%" height="50%"/>

### [(AAAI2022) Rank Mimicking and Prediction-guided Feature Imitation](https://ojs.aaai.org/index.php/AAAI/article/download/20018/version/18315/19777)

<img src="https://user-images.githubusercontent.com/66883050/216758291-974c72ff-184f-455e-bfb4-7ac5038b1782.png"  width="50%" height="50%"/> <img src="https://user-images.githubusercontent.com/66883050/216758296-a4f160bc-3cda-45c3-afbc-ac7a75666ada.png"  width="50%" height="50%"/>


### [(ECCV2022) Prediction-Guided Distillation](https://arxiv.org/abs/2203.05469)


### [(ICLR2023 accepted) Masked Distillation with Receptive Tokens](https://arxiv.org/abs/2205.14589)


![map](https://user-images.githubusercontent.com/66883050/216757389-57061776-ac90-4545-8c97-42bcdfaa4d5e.PNG)
