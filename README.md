# Awesome Knowledge Distillation for Object Detection

### research flow
![map](https://user-images.githubusercontent.com/66883050/216807767-c81a9939-6fce-4bdc-a68d-d8aad53989ad.PNG)


### [(CVPR2017) Mimicking very efficient network for object detection](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8100259)

<img src="https://user-images.githubusercontent.com/66883050/216758223-a1767b16-e3b4-42ab-ae23-d5ed9800dd96.png"  width="40%" height="40%"/>

### [(CVPR2019) Distilling object detectors with fine-grained feature imitation](https://arxiv.org/abs/1906.03609)

<img src="https://user-images.githubusercontent.com/66883050/216758237-e6a27300-73d0-43dc-8f36-c9a24ad84f92.png"  width="40%" height="40%"/>

### [(CVPR2021) General instance distillation for object detection](https://arxiv.org/abs/2103.02340)

<img src="https://user-images.githubusercontent.com/66883050/216758253-63da5c2d-659f-4072-af41-3fcbe0970c7f.png"  width="50%" height="60%"/>

### [(CVPR2021) Distilling object detectors via decoupled features](https://arxiv.org/abs/2103.14475)

<img src="https://user-images.githubusercontent.com/66883050/216758264-d4aa0073-4e45-4094-adac-870e889b6261.png"  width="50%" height="60%"/>

### [(NeurIPS2021) Distilling object detectors with feature richness](https://arxiv.org/abs/2111.00674)

<img src="https://user-images.githubusercontent.com/66883050/216758273-be09d30b-6184-4ef5-a591-e3d70ea5ea20.png"  width="50%" height="50%"/>

### [(CVPR2022) Focal and global knowledge distillation for detectors](https://arxiv.org/abs/2111.11837)

<img src="https://user-images.githubusercontent.com/66883050/216758284-215859ae-68e9-4f0d-9764-09b0c22484ca.png"  width="50%" height="50%"/>

### [(AAAI2022) Rank Mimicking and Prediction-guided Feature Imitation](https://ojs.aaai.org/index.php/AAAI/article/download/20018/version/18315/19777)

<img src="https://user-images.githubusercontent.com/66883050/216808295-704563c1-33cc-45c0-95ac-4fe7c1b0050d.png"  width="75%" height="75%"/>

### [(ECCV2022) Prediction-Guided Distillation](https://arxiv.org/abs/2203.05469)

<img src="https://user-images.githubusercontent.com/66883050/216807772-a03530f3-7e0f-4549-9659-19cf02586319.png"  width="60%" height="60%"/> 


### [(ICLR2023 accepted) Masked Distillation with Receptive Tokens](https://arxiv.org/abs/2205.14589)


<img src="https://user-images.githubusercontent.com/66883050/216807831-17fbf398-9fd1-4f18-81d6-5ce58693284f.png"  width="60%" height="60%"/> 


# References

